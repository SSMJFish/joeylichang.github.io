# NoSQL分布式系统概述
本篇文章目的介绍NoSQL分布式存储系统在设计上需要具备的基础知识，从比较高的角度审视一个NoSQL分布式存储系统，并不会设计过多细节（具体细节在其它blog中介绍）。

## 背景
随着互联网业务的发展单机单副本的数据库架构不再满足需求，采用分布式的集群解决方案几乎是必然的结果。NoSQL凭借着接口简单、更高的性能等优势（牺牲了事务、强一致性）迅猛的发展，同时NoSQL分布式系统也带来了很多新的设计方式、算法、理论等。

分布式系统最终目的都是[高可用](https://zh.wikipedia.org/wiki/%E9%AB%98%E5%8F%AF%E7%94%A8%E6%80%A7)，高可用是一个很大的话题也很难全面的阐述，针对不用的具体场景其体现也是不尽相同。本篇文章通过简单的描述业务对NoSQL分布式系统的显示需求和隐式需求引出NoSQL分布式系统设计的出发点，并针对每个点进行概要性的阐述。

首先，数据如果是单副本（node）在单机故障、进程故障、进程假死等情况下，数据可靠性、数据可用性都会受到挑战。多副本（nodes）架构是单副本架构的自然扩展（即水平扩展），多副本的出现自然引出副本间数据一致性问题。

其次，随着数据量的增加单分片（Replication）的容量和性能都会成为瓶颈，扩展单分片架构为多分片（Replications）架构（即垂直扩展，每个分片负责存储部分数据）也是自然。数据如何分配到不同的分片直接影响数据的搬迁，即扩缩容、均衡等，数据搬迁的选择对于数据可靠性、可用性、可运维性至关重要。

最后，随着单副本向多副本、单分片向多分片发展集群规模也在不断扩张，系统需要一个协调者的角色，协调者主要负责节点故障探测、主从切换、运维操作的发起者甚至是执行等任务。协调者可以是中心化的（单独的进程模块），也可以是去中心化的（Gossip），如果是中心化的协调者方案，协调者可能是一个一主多从的分布式小集群，也可能是多个分布式集群的集合（例如Amazon S3），但不太可能是一个单点（主要是考虑可靠性）。

总结一下，一个NoSQL分布式集群在设计上应该考虑一下三点：

1. 分片内数据一致性。
2. 数据与物理节点之间如何映射。
3. 系统协调方案。

## 数据一致性
一致性可以分为强一致性和弱一致性，本文从这个分类分别介绍一下常见的一致性算法，之后根据CAP（Consistency，Availability，Partition tolerance）理论描述一下他们俩之间的取舍。

### 强一致协议
* Paxos算法
	* Google Chubby的作者Mike Burrows说过这个世界上只有一种一致性算法，那就是Paxos，其它的算法都是残次品。
	* Paxos算法的目的是协调多个节点间数据达成一致，由于论文晦涩难懂并且论文中没有明确的实现步骤，几乎使得每一个Paxos的实现多多少少都有一些变形（基本也快成为业界共识），所以社区没有公认的项目完美的实现了Paxos，导致社区中使用的并不多。
	* Paxos是唯一一个多主的强一致协议，即副本间不需要区分主从，详细介绍见[Paxos概述](https://blog.csdn.net/weixin_41713182/article/details/88147487)。

* Raft算法
	* Raft是基于Paxos算法的改进，增加了选主逻辑，在日志复制和配置更新等方面做了一些约束。
	* Raft凭借着实现简单（算法本身易于理解、论文中给出了明确的实现）被社区广发使用，在中心节点集群、NewSQL等场景中有很多应用（详细介绍见后续bolg）。

* NWR算法
	* N是副本数，W是写操作最少成功的副本数，R是读操作至少成功的副本数。
	* W+R > N，可以保证读写都是基于最新的数据，一般是根据具体场景在W/R数值上做权衡。

* 复制链
	* 复制链的方式首次出现在GFS的论文中，主要是考虑优化主节点扇出带宽而设计。
	* 假设有A、B、C三个节点，客户端更新A，A复制数据给B，B复制数据给C，C最后发送ACK给A表述所有副本收到了数据 ，A返回客户端更新成功。
	* GFS支持大文件追加写，在写入带宽较大时复制链的方式保证每个节点扇出的带宽平均，如果是A复制给B、C那么A的扇出带宽交大，B、C较小造成不均衡。

### 弱一致协议
* 主从异步同步数据
	* 写主成功即返回客户端成功，异步同步数据给备份分片。
	* 选择这种方式多数是性能优先的业务，对数据丢失有一定的容忍度，常见于Cache。

* 反熵算法
	* 反熵算法的思想周期性的对比节点间数据的差异进行数据补偿，从而保证节点数据都是最新的数据，常见有三种方式:
		1. Push：A发送数据摘要给B，B经过计算发送差异化数据摘要给A，A将差异化数据给B用于更新。
		2. Pull：A发送数据摘要给B，B经过计算将差异化数据给A用于更新。
		3. Pull/Push：A发送摘要给B，B发送差异化数据给A用于更新，同时发送B旧数据的摘要，A更新本地数据之后发送新数据给B用于B更新。
		4. 从效率上看，Pull && Push > Pull > Push。
	* 反熵协议的一致性相对较差，主要用于集群拓扑信息或者一些元数据之中。
	* 反熵协议是一种去中心化的协议，即不区分主从，优点是集群的容错性取决于集群大小即集群越大容错性越强（少于半数节点故障都可以正常工作），如果是中心化的协议，主故障期间服务降级甚至不可用。
	* 如果集群不是很大信息收敛时间较为理想，同时提升了容错性。如果集群规模较大信息收敛时间过长对于服务还是会有一定影响。至于集群规模多大算大这要看具体实现的效率。

### 其他方式
除了上述强一致协议、弱一致协议之外还可能有其他的方式管理副本内的数据，典型的编码方式就是一种，比如EC编码、RS编码，在Amzon S3中就使用了RS编码。简单描述就是将数据切分为n块，经过RS编码之后生成了m块校验数据，m+n块数据中只要保证至少n块数据正常即可恢复用户数据。一般采用编码方式处理分片内的数据主要目的是节省成本，比如n= 8，m=4可以做到1.5副本比常见的三副本、两副本节省了一定资源。

### 一致性算法的选择
一致性算法的选择需要看具体的业务需求，但是本质上都是CAP理论作为理论基础。CAP即Consistency，Availability，Partition tolerance，CAP理论简单描述是三者不可同时达到最多取两者，在实际场景中网络分区是不可避免的，大多数都是在C、A之间做取舍。

C（即一致性）得到了保证，比如使用强一致协议Raft性能上相对弱一致协议的主从异步同步方式至少差1个量级，并且在大多数节点故障时不可写那么A（即可用性）就会被舍弃。相反如果采用弱一致协议的主从异步同步方式A得到了保证，C就会被舍弃。


## 数据与节点的映射
NoSQL最基础的接口是key-value格式，直接记录key与节点的映射关系显然是不现实的。一般是将key分段进行管理，一个节点对应如果key的分段。这样做的好处有一下几点：

1. 显然分段会比节点多，可以保证分段在节点上均匀分布（不代表数据量、访问量是均匀分布的）。
2. 按分片进行数据迁移，可以容易实现对用户透明迁移。

本部分主要介绍数据与节点映射的算法，在这之前会介绍一下常见的数据迁移方案，因为数据迁移在系统运行阶段至关重要直接影响系统的扩展性、可运维性，而数据迁移与数据与节点的映射方式直接相关，所以放在此部分进行介绍。

### 数据迁移
NoSQL分布式系统的数据迁移大致可以分为两类：

1. 迁移期间删除源数据。
2. 迁移全部完成之后删除源数据。

* 迁移期间删除源数据的方案典型的是RedisCluster方案，每次批量搬迁部分key（几个~几百）搬迁是一个原子操作直到搬迁成功之后删除原数据，搬迁期间正在搬迁的key不接收访问返回try_again，RedisCluster提供Asking、Moving等重定向语义保证用户对已经搬迁或者新数据一定在目标上操作。

这个方案的有点是对用户的请求几乎透明，之所以是几乎是因为正在搬迁的key返回try_again。缺点是搬迁期间一旦目标节点发生主从切换，可能有部分搬迁数据丢失，并且搬迁速度较慢（为了减少对用户的影响，一次迁移的key数量有限）。

* 迁移完成之后删除源数据的方案在一些对数据可靠性要求比较高的系统中用的比较多。基本思想是，先dump源数据发送给目标节点，dump期间的请求会落盘作为oplog，在dump完成之后发送之前积攒的oplog给目标节点，同步oplog期间搬迁的数据分段不可写否则oplog可能一直追赶不上。

这个方案的优点是目标故障之后源节点还会保存全量的最新数据不会丢失最近的更新，批量dump元数据相比之前方案一次搬迁一小部分key速度会快很多。缺点对用户不够友好，因为追oplog期间数据不可写，另外如果在紧急扩容（源节点数据快写满了），很可能在搬迁完成之前源节点写满而拒绝源节点上所有数据分段的写请求。

### 动态调整分段
数据段的划分方式自然会相对一段连续key对应一个节点，或者是对key进行hash取摸完成映射。这些方式在NoSQL系统中都比较常见，比如RedisCluster采用crc(key)%16384（即数据划分为16384个数据段），还有一些NoSQL系统key是unsign int类型并且可以无限增长。

在这里介绍一种一致性Hash的数据映射方式，一致性Hash在Google有很多介绍这里不再赘述了。key经过一致性hash计算之后映射到一个虚拟节点，虚拟节点所在的物理节点之后连续的两个物理节点作为副本，利用一致性hash算法的优点节点的加入离开都会尽可能小的均匀的影响其他节点，附加暗示提交技术可以很好的保证数据恢复时间（DynamoDB方案后续Blog介绍）。

### 多属性进行数据分段
之前介绍的数据与物理节点的映射方式都是针对key一个维度进行的，有一些系统中针对key-value还有一些额外的属性信息，出于业务的要求需要相同或者相似属性的数据存储在一台机器上，那么数据段的划分方式就不能仅仅考虑key一个维度了。

多属性可以被认为是是一个空间维度（超过两个属性就是一个多维空间），数据分段映射为一个多维空间的超平面，一个超平面对应一个节点，一个节点对应多个超平面。这个方案的缺点是随着属性的增减划分的超平面成指数级增长。一般NoSQL系统中支持一个meta的额外信息用于描述key。

## 系统协调者
集群中节点探活需要一个角色做出最终判断，分片内副本选主（需要借助外部选主的协议）需要一个角色指定，分片内主副本故障需要一个角色进行主从切换，扩缩容任务的制定与执行也需要一个角色执行等等上述情况都可以概括为需要一个角色根据系统的资源、拓扑结构等具体情况作出协调与处理，在这里我们统一称作系统的协调者吧。

针对协调者有几点需要说明：

1. 协调者是一个统称，并不是一个协调者负责所有的协调工作，可能有多种类型的协调者比如有负责主从切换的协调者，有负责数据搬迁的协调者，他们可以是一个模块也可以是两个。
2. 协调者并不仅仅针对中心化集群解决方案，在去中心化集群中也需要，例如RedisCluster，主从切换时只有分片的主节点可以参与投票，此时所有的主节点就是协调者。
3. 协调者的形式可以是一个有奇数个几点组成的小集群（基于强一致协议）、也可以是一主一备（Shadow）的简单架构、也可以是若干个分布式集群组成（例如Amazon S3）、更可以整个数据集群（Gossip协议的集群）等等。

分布式系统协调的概念比较大，本文先对协调者常见的一些方案进行一些梳理，然后针对NoSQL中比较重要（基本所有的NoSQL集群都需要考虑的）的三个点进行介绍，即节点状态管理、主从切换、数据迁移。

### 节点状态管理
节点状态管理主要负责节点的探活、节点的上下架、节点信息收集、拓扑信息维护等基础的元信息管理与维护，基于以上信息自动发起任务让系统运行的更健康，同样基于以上元信息支持基础的运维操作。元信息的传播（对等关系）、上报（中心化）几乎都是通过心跳，心跳除了传播、上报元信息之外，还有一个重要作用就是探活，判断节点是否正常，如果心跳超时需要进行处理。下面看一下目前常见的心跳解决方案：

* double check：中心化解决方案（后面介绍）中，master节点通过心跳收集每个节点的信息并汇总成集群的元数据，如果一次超时，为了防止误判会隔断时间重试确认节点是否故障。
* back-up check：在double check方案中如果只是心跳之间两个节点网络短暂时间故障，有误判可能（增加重试次数不是根本解决办法），在中心化解决方案中考虑到高可用一般会部署多个master（一主多从），back-up check方案是当主master判断一个节点故障之后会询问从master的意见（具体实现上可以让从master也收集心跳，或者等主master询问时再去与节点通信），做出最后决策，这样可以避免之前问题，甚至可以避免网络分区的问题（主master与节点网络分区）。
* 少数服从多数：back-up check方案中主master可以收集所有从master的意见如果超过半数认为节点故障就判断为故障，在这里介绍的是RedisCluster的去中心化的探活方案，如果从节点判断主节点故障会向其他分片的主节点发起投票请求（所有节点间有心跳），如果收到半数以上的投票，则从节点提升为主。
* 动态调整超时：记录之前若干条心跳到达时间，根据概率分布（例如正态分布）判断下一次心跳的到达时间。优点是具备一定的自适应能力，对于网络延时、节点状态有更好的适应能力相对固定的超时时间。

### 主从切换
分片内节点的一致性协议决定了系统协调者在分片内选主中起到的作用。
* 强一致协议：例如Raft协议本身具备选主的功能，协调者不需要参与选主。在心跳检测中一般也会上报节点的主从角色，这样可以更好的指导客户端高效的访问。对于Paxos协议所以没有主从的概念，但是在Mutli-Paxos中如果可以知道proposerID指代的机器可以大大的减少活锁的可能性。
* 弱一致协议：对于向异步同步数据的方式，选主完全有协调者做出判断，一般会选择数据最新的从提升为主，当然选主策略是可以定制的比如考虑机房、地域等因素。

### 数据搬迁
前面已经介绍过了常见的NoSQL数据搬迁方案，在这里说一下数据搬迁的执行者。数据搬迁的执行者可以是master，也可以是另一个模块根据从master获得信息自动发起搬迁，更可以是master执行搬迁任务下发给一个模块执行。具体选择哪一种方式要看具体情况。如果搬迁的数据量不大并且master性能足够完全可以由master执行。如果集群规模较大，元数据很多，或者master性能是瓶颈，建议将搬迁模块分离开。

### 协调者方案
上面介绍了一些常见的协调者功能上的解决方案，最后介绍一些常见的协调者方案有哪些。

* Master-Shadow：一主一备架构，Shadow完全是Master的镜像，缺点是一旦Master故障之后需要手动切换Shadow为主（或者外围模块执行切换）。
* 一主多从：协调者必须具备唯一的主，因为大部分的系统内部的请求（数据搬迁、祝从切换等）具有唯一性。一主多从的架构类似数据的一个分片（很多项目的中心节点就是一个数据分片），选主的方式类似之前介绍的数据一致性内容。
* 跨地域：上述两种方案中在集群跨地域部署时由于跨地域网络不稳定因素，在心跳探测和master间数据同步都可能有很高的延时，一种常见的解决方案是每个地域不是一套Master集群，从地域的master集群只负责收集本地域的节点信息，从地域的主master向主地域的主master批量汇报，由主地域的主master做出唯一的判定和决策。

最后，说一下数据节点如果是采用Gossip协议这样的去中心化协议，也是会需要中心节点收集信息（用于信息展示、运维操作）、搬迁数据等，所以也是需要协调者角色的。
