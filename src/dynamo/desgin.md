# Dynamo Desgin

本文依据之前的文章（[NoSQL分布式系统](https://github.com/joeylichang/joeylichang.github.io/blob/master/src/nosql_desigin/nosql_distributed_systems_desgin.md)）思路从数据分布、数据一致性、节点管理三个维度介绍Dynamo的分布式架构设计，最后分析一下DynamoDB是如何做到高扩展、高可用、异地多活等特点。

## 数据分布
Dynamo在数据分布上使用了[一致性哈希](https://zh.wikipedia.org/zh-hant/%E4%B8%80%E8%87%B4%E5%93%88%E5%B8%8C)，一个节点使用负责一部分虚拟节点，每个节点负责的虚拟节点不连续，每个虚拟节点负责的key range相等，一致性哈希网上的资料很多不在此赘述。Dynamo选择一致性哈希作为数据在各节点上的分配策略主要是保证了系统的高扩展，可以在系统运行阶段进行扩缩容（配合草率仲裁和暗示移交后续介绍），除此之外一致性哈希引入的虚拟节点还有以下优点：
1. 离开节点的负载将均匀地分散在剩余的可用节点，加入节点接受来自其他可用的每个节点的负载量大致相当。
2. 可兼容物理节点的性能差异调整物理节点负责的虚拟节点数量
3. 可根据数据量和节点负载动态调整物理节点负责的虚拟节点数量。

## 数据一致性 

#### Replicate组成
Dynamo的Replicate内部不区分主从全部可以作为主用于数据的写入，key经过一致性哈希计算后获得虚拟节点再根据路由定位到物理节点进行写入，从一致性哈希计算得来的虚拟节点开始顺延N - 1个虚拟节点（跳过重复的物理节点）组成一个Replicate。

请求会路由给Replicate（论文中称为首选列表）中第一个虚拟节点也称为协调节点，协调节点负责节点的写入和读取。Replicate中如果有节点故障不可达时会跳过顺延下一个虚拟节点（请求会落在Replicate之外的节点，后续介绍暗示移交技术定时将非本地数据同步给Replicate内的节点），如果第一个节点（协调节点）故障同样会顺延。

#### 一致性协议
Dynamo在Replicate内的一致性协议使用的是NWR协议，即至少读R个节点全部成功才算成功，至少写W个节点才算成功，Replicate内一共N各节点（配置）且W+R>N。W和R的设置会影响可用性、可靠性、一致性，例如W=1，系统不会拒绝写请求（除非全部节点故障），低的W和R值会增加不一致性的风险，因为写请求被视为成功并返回到客户端，即使他们还未被大多数副本处理。

#### 写冲突解决
Dynamo支持多主，即可以同时向Reclipate内多个节点写同一个key。解决冲突的办法是MVCC（多版本控制），MVCC使用向量时钟实现，一个key会维护向量时钟列表，向量时钟可以标识更新之间是因果关系还是平行关系，如果是因果关系可以直接覆盖（满足偏序关系），如果是平行关系将所有的向量时钟信息返回给客户端由业务处理。将冲突交个业务处理，增加了业务的负担，论文给出的例子是购物车，即如果因为节点临时故障导致购物车删除和添加导致购物车内物品不一致，业务在处理时会忽略删除的物品将用最多的物品数据作为最终数据写入Dynamo，虽有会有漏删现象但是对于业务是可以接受的。

Dynamo还支持基于时间戳的协调，不同于之前购物车的案例，在出现不同版本的情况下，Dynamo执行简单的基于时间戳的协调逻辑：“最后的写获胜”，也就是说，具有最大时间戳的对象被选为正确的版本。一些维护客户的会话信息的服务是使用这种模式的很好的例子。

目前常见的解决写冲突的方案：
1. [CRDT](https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type)：需要数据结构上的支持并不是所有的数据结构都会支持并且也需要更新操作满足特定条件，开源社区使用的并不多，商业版Redis实现了一套CRDT的数据结构，但是对Redis的实现进行了彻底的改动（目前也未开源）。
2. HLC时钟算法：采用物理时钟 + 逻辑时钟的方式解决冲突，因为物理时钟有跳变的情况，引入逻辑时钟解决该问题，保证更新具备偏序关系，从而解决合并冲突问题。
3. 新硬件 + 算法：Intel有一款CPU可以保证物理时钟向前不回退，但这是保证单机的情况，还需要附加算法保证集群（机器间）时钟向前。问题是该物理时钟与现实世界的时钟存在误差，一般用于全局ID生成器，比如NewSQL的事务协调器需要全局唯一且递增的事务ID。
4. 原子时钟：Spanner使用的方式，需要硬件支持。

#### 节点临时故障
如果节点临时故障（网络不可达、负载过高导致超时等），还按着Replicate内的节点进行更新势必会影响可用性，为此Dynamo使用了草率仲裁 + 暗示移交技术解决。草率仲裁指的是如果Replicate内有节点不可能会跳过该节点比如第一个节点（协调节点故障）将会把发送给他的数据范送给Replicate之后顺延的一个虚拟节点。暗示移交指的是，当顺延的节点收到不应该属于他的时候时（请求中带有暗示字段），会在本地创建一个临时存储保存数据并记录暗示对象（Replicate的第一个节点），后续会探测暗示节点的状态，如果正常会将数据发送给他并删除本地数据。

使用暗示移交，确保读取和写入操作不会因为节点临时或网络故障而失败。需要最高级别的可用性的应用程序可以设置W为1，这确保了只要系统中有一个节点将key已经持久化到本地存储 ,　一个写是可以接受(即一个写操作完成即意味着成功)。因此，只有系统中的所有节点都无法使用时写操作才会被拒绝。

Dynamo系统具备处理整个数据中心故障的能力（断电，冷却装置故障，网络故障和自然灾害发生故障），Dynamo可以配置成跨多个数据中心地对每个对象进行复制。从本质上讲，一个key的首选列表的构造是基于跨多个数据中心的节点的。

#### 节点永久故障
暗示移交在系统在节点短暂的失效的情况下工作良好，但是无法解决节点彻底或者长时间故障无法加回集群的场景，Dynamo实现了反熵协议来保持副本同步。为了更快地检测副本之间的不一致性，并且减少传输的数据量，Dynamo采用[MerkleTree](https://en.wikipedia.org/wiki/Merkle_tree)（详情连接不在此赘述）。每个节点为它承载的每个key range(由一个虚拟节点覆盖 key 集合)维护一个单独的MerkleTree。两个节点交换MerkleTree的根，对应于它们承载的共同的键范围。其后，使用上面所述树遍历方法，节点确定他们是否有任何差异和执行适当的同步行动。

## 节点管理

Dynamo采用去中心化方式管理集群，即没有中心集群（master集群管理），使用[Gossip协议](https://github.com/joeylichang/joeylichang.github.io/blob/master/src/distributed_protocol/gossip/overview.md)（之前文章介绍过不在此赘述）。每个节点维护全量的物理节点与虚拟节点的对应关系，论文中介绍集群规模最大在几百台上千台之后Gossip协议收敛时间仍不是客观的，这点与之前的介绍文章的线上实际运行情况也是相符的。Dynamo在为了加快新节点的信息传播引入了seeds节点概念，即每个节点都会与seeds节点交互信息，不会出现两个几点同时间如时彼此"不认识"的情况。

Dynamo能够支持跨地域部署也要归功于去中心化的协议，如果是中心化解决方案跨地域部署很难解决跨地域网络抖动（比较频繁），这个在之前的文章也有介绍不在此赘述。


## 其他
Dynamo论文从发布到现在有10年以上，从仅支持内部系统到AWS售卖一定有很多的演进和优化。由于其为开源也很难了解其具体的演进过程和具体实现，下面总结一些论文和其他资料中的信息：
1. Dynamo是java实现，GC应该是一个不可避免的问题，Cassandra被认为是Dynamo的开源实现（引擎部分应该是不一样的）GC一直是社区抱怨最多的问题。
2. Dynamo支持多种存储引擎，其中包括纯内存的引擎。
3. Dynamo节点区分前后台任务，前台任务是用户的get/put请求，暗示移交、Gossip等是后台任务，并对前后台任务进行了隔离，实时采集前台任务的信息反馈给后台任务控制器。
4. Dynamo支持proxy和节点转发两种模式，前者耗时比较比较高（网络上多一跳），但是对于一些请求监控信息采集和运维操作比较友好。

## 总结
1. 高度可扩展：一致性hash。
2. 高可靠：临时故障（草率仲裁 + 暗示移交） + 永久故障（Merkle树的反熵）。
3. 高性能：前后台任务分离 + 支持全内存引擎 + 草率仲裁 + 暗示移交。
4. 跨地域部署：Gossip协议 + Replicate跨地域部署。
5. 多主：MVCC。
6. 不停写：草率仲裁 + 暗示移交。
